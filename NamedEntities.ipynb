{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUkWAQ721mYp"
   },
   "source": [
    "<b><h2>Reconocimiento de Entidades Nombradas utilizando NLTK y SpaCy</h2></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrY2a2jr1mZE"
   },
   "source": [
    "El reconocimiento de entidades nombradas es probablemente el primer paso hacia la extracción de información que busca localizar entidades nombradas que se encuentran dentro del texto y clasificarlas en categorías predefinidas tales como personas, organizaciones, lugares, expresiones de tiempo (fechas), cantidades, valores monetarios, porcentajes, etc.\n",
    "\n",
    "El reconocimiento de entidades nombradas tiene muchas aplicaciones dentro del procesamiento de lenguaje natural y puede ayudar a resolver muchas preguntas como:\n",
    "\n",
    "<ul>\n",
    "<li>Qué compañias son mencionadas en un artículo o noticia\n",
    "<li>La mención de productos específicos en alguna reseña o comentario\n",
    "<li>El nombre de personas y su localización\n",
    "</ul>\n",
    "\n",
    "Pues bien, como es costumbre, comenzaremos por importar las librerías necesarias. Comenzaremos trabajando con NLTK y posteriormente trabajaremos con SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hYq8GqbV1mZF"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N2AWiF4r3jDa",
    "outputId": "cc9dc4be-1070-4645-c37e-d5c001dce875"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jumunoz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\jumunoz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\jumunoz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\jumunoz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping help\\tagsets.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7JQvjJS1mZH"
   },
   "source": [
    "Una vez que se han cargado las librerías, se toma un texto para comenzar a probar. pare este ejemplo se ha tomado un texto obtenido de una noticia.\n",
    "\n",
    "'European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DeuxOExM1mZI"
   },
   "outputs": [],
   "source": [
    "text = 'European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EyGbY9OC1mZJ"
   },
   "source": [
    "Se aplica tokenization y part-of-speech para generar etiquetas para las palabras en la oración. Para ello se define una función que recibe como parámetro el enunciado (sent) y le aplica ambos métodos.\n",
    "\n",
    "El resultado de esta función muestra una lista de palabras (tokens) con su clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "veX-8fWZ1mZK"
   },
   "outputs": [],
   "source": [
    "def preprocess(sent):\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2I0O8sXm1mZL",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('European', 'JJ'),\n",
       " ('authorities', 'NNS'),\n",
       " ('fined', 'VBD'),\n",
       " ('Google', 'NNP'),\n",
       " ('a', 'DT'),\n",
       " ('record', 'NN'),\n",
       " ('$', '$'),\n",
       " ('5.1', 'CD'),\n",
       " ('billion', 'CD'),\n",
       " ('on', 'IN'),\n",
       " ('Wednesday', 'NNP'),\n",
       " ('for', 'IN'),\n",
       " ('abusing', 'VBG'),\n",
       " ('its', 'PRP$'),\n",
       " ('power', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('mobile', 'JJ'),\n",
       " ('phone', 'NN'),\n",
       " ('market', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('ordered', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('company', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('alter', 'VB'),\n",
       " ('its', 'PRP$'),\n",
       " ('practices', 'NNS')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = preprocess(text)\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hq5kwNiCStz1",
    "outputId": "f4944a47-bd13-4eb0-dc82-1c6317d81a99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('NNS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_RHnENdKS5HT"
   },
   "source": [
    "https://www.learntek.org/blog/categorizing-pos-tagging-nltk-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSm6HgdL1mZO"
   },
   "source": [
    "Lo que sigue es agrupar las frases nominales (grupo de palabras cuyo núcleo está constituido por un sustantivo) para identificar entidades nombradas. Para ello utilizaremos una expresión regular que define las reglas que indican cómo se deben obtener los fragmentos de una oración.\n",
    "\n",
    "Ejemplo de frase nominal El hijo de la vecina es muy inteligente.\n",
    "\n",
    "El patron para formar fragmentos consiste en una regla que establece que una frase nominal (NP) debe formarse cada vez que se encuentre un <a href=\"https://grammar.yourdictionary.com/parts-of-speech/nouns/what/what-is-a-determiner.html\">determinante opcional</a> (DT) seguido de cualquier número de adjetivos (JJ), y por último un sustantivo (NN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Et7xLeoF1mZO"
   },
   "outputs": [],
   "source": [
    "pattern = 'NP: {<DT>?<JJ>*<NN>}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPWrFCEs1mZP"
   },
   "source": [
    "Utilizando este patrón se crea un parser de fragmentos y se prueba con el enunciado de ejemplo.\n",
    "En la impresión que se genera como resultado podemos ver que, siguiendo el patrón establecido se forman fragmentos como:\n",
    "\n",
    "(NP a/DT record/NN)\n",
    "(NP power/NN)\n",
    "(NP the/DT mobile/JJ phone/NN)\n",
    "(NP the/DT company/NN)\n",
    "\n",
    "Los cuales están conformados por los elementos establecidos en la expresión regular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1y7LeX2Z1mZP",
    "outputId": "62180119-128f-4223-a801-c9b17ef7a872",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  European/JJ\n",
      "  authorities/NNS\n",
      "  fined/VBD\n",
      "  Google/NNP\n",
      "  (NP a/DT record/NN)\n",
      "  $/$\n",
      "  5.1/CD\n",
      "  billion/CD\n",
      "  on/IN\n",
      "  Wednesday/NNP\n",
      "  for/IN\n",
      "  abusing/VBG\n",
      "  its/PRP$\n",
      "  (NP power/NN)\n",
      "  in/IN\n",
      "  (NP the/DT mobile/JJ phone/NN)\n",
      "  (NP market/NN)\n",
      "  and/CC\n",
      "  ordered/VBD\n",
      "  (NP the/DT company/NN)\n",
      "  to/TO\n",
      "  alter/VB\n",
      "  its/PRP$\n",
      "  practices/NNS)\n"
     ]
    }
   ],
   "source": [
    "cp = nltk.RegexpParser(pattern)\n",
    "cs = cp.parse(sent)\n",
    "print(cs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3uJDS8Bl1mZf"
   },
   "source": [
    "<b><h1>Etiquetas IOB</h1></b>\n",
    "\n",
    "Las <a href=\"https://www.geeksforgeeks.org/nlp-iob-tags/\">Etiquetas IOB</a> se han convertido en una forma estandar de representar fragmentos de texto.\n",
    "\n",
    "Estas etiquetas son similares a las que se obtienen por medio de part-of-speech pero pueden decir el inicio, interior y exterior de un fragmento. No solo permiten frases nominales sino también otro tipo de frases.\n",
    "\n",
    "En el siguiente ejemplo se importan las librerías para la obtención de estas etiquetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xuff5cEk1mZf",
    "outputId": "f22ea73b-5792-45eb-ad70-59fb421eb2d4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('European', 'JJ', 'O'),\n",
      " ('authorities', 'NNS', 'O'),\n",
      " ('fined', 'VBD', 'O'),\n",
      " ('Google', 'NNP', 'O'),\n",
      " ('a', 'DT', 'B-NP'),\n",
      " ('record', 'NN', 'I-NP'),\n",
      " ('$', '$', 'O'),\n",
      " ('5.1', 'CD', 'O'),\n",
      " ('billion', 'CD', 'O'),\n",
      " ('on', 'IN', 'O'),\n",
      " ('Wednesday', 'NNP', 'O'),\n",
      " ('for', 'IN', 'O'),\n",
      " ('abusing', 'VBG', 'O'),\n",
      " ('its', 'PRP$', 'O'),\n",
      " ('power', 'NN', 'B-NP'),\n",
      " ('in', 'IN', 'O'),\n",
      " ('the', 'DT', 'B-NP'),\n",
      " ('mobile', 'JJ', 'I-NP'),\n",
      " ('phone', 'NN', 'I-NP'),\n",
      " ('market', 'NN', 'B-NP'),\n",
      " ('and', 'CC', 'O'),\n",
      " ('ordered', 'VBD', 'O'),\n",
      " ('the', 'DT', 'B-NP'),\n",
      " ('company', 'NN', 'I-NP'),\n",
      " ('to', 'TO', 'O'),\n",
      " ('alter', 'VB', 'O'),\n",
      " ('its', 'PRP$', 'O'),\n",
      " ('practices', 'NNS', 'O')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from pprint import pprint\n",
    "iob_tagged = tree2conlltags(cs)\n",
    "pprint(iob_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RtRkfgm21mZg"
   },
   "source": [
    "En esta resultado se puede ver que hay un token por cada línea, cada uno con su etiqueta de part-of-speech y su etiqueta de entidad nombrada.\n",
    "\n",
    "Teniendo como base este corpus de entrenamiento, se puede construir un etiquetador que puede ser usado para etiquetar nuevos enunciados, y utilizar la función nltk.chunk.conlltags2tree() para convertir la secuencia de etiquetas en un árbol de fragmentos.\n",
    "\n",
    "Con la función nltk.ne_chunk() se pueden reconocer entidades nombradas utilizando un clasificador que agregue etiquetas de categoria como PERSON, ORGANIZATION, GPE (Geo-Political Entities (ciudades, estados, provincias, países)), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ks_JK-eJYMfF",
    "outputId": "1988fcae-7dcf-40d4-b521-56f70a37e381"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\jumunoz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\jumunoz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aOZSbQNp1mZg",
    "outputId": "751c78b4-0d56-41ff-e976-226efb7c3f96",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE European/JJ)\n",
      "  authorities/NNS\n",
      "  fined/VBD\n",
      "  (PERSON Google/NNP)\n",
      "  a/DT\n",
      "  record/NN\n",
      "  $/$\n",
      "  5.1/CD\n",
      "  billion/CD\n",
      "  on/IN\n",
      "  Wednesday/NNP\n",
      "  for/IN\n",
      "  abusing/VBG\n",
      "  its/PRP$\n",
      "  power/NN\n",
      "  in/IN\n",
      "  the/DT\n",
      "  mobile/JJ\n",
      "  phone/NN\n",
      "  market/NN\n",
      "  and/CC\n",
      "  ordered/VBD\n",
      "  the/DT\n",
      "  company/NN\n",
      "  to/TO\n",
      "  alter/VB\n",
      "  its/PRP$\n",
      "  practices/NNS)\n"
     ]
    }
   ],
   "source": [
    "ne_tree = nltk.ne_chunk(pos_tag(word_tokenize(text)))\n",
    "print(ne_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OfEwXN1d1mZh"
   },
   "source": [
    "<b><h1>SpaCy</h1></b>\n",
    "\n",
    "El reconocimiento de entidades utilizando la librería SpaCy ha sido entrenado utilizando el corpus de <a href=\"https://catalog.ldc.upenn.edu/LDC2013T19\"> OntoNotes5</a> y soporta las siguientes <a href=\"https://spacy.io/api/annotation#section-named-entities\"> entidades</a>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "eNDXT45_1mZh"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztJBPGkH1mZh"
   },
   "source": [
    "Para este ejemplo vamos a usar el mismo enunciado.\n",
    "\n",
    "Una de las cosas más interesantes de SpaCy es que es un poco más simple.\n",
    "\n",
    "El enunciado se envía como parámetro a la función nlp() y posteriormente se puede imprimir como resultado la clasificación de cada entidad nombrada que reconozca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8G4O1g4K1mZi",
    "outputId": "7dcb9da8-76f8-40f5-9794-ae993e107921",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('European', 'NORP'), ('Google', 'ORG'), ('$5.1 billion', 'MONEY'), ('Wednesday', 'DATE')]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices')\n",
    "print([(X.text, X.label_) for X in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evWY03181mZi"
   },
   "source": [
    "Se puede ver a diferencia del ejemplo anterior que tiene mayor precisión al momento de reconocer entidades, ya que por ejemplo 'European' la ha clasificado como NORP (nationalities or religious or political groups) y Google la ha clasificado como organization y no como PERSON.\n",
    "\n",
    "<b><h1>Tokens</h1></b>\n",
    "\n",
    "Hasta ahora hemos trabajado con reconocimiento a nivel entidad. En el siguiente ejemplo vamos a trabajar con reconocimiento a nivel token utilizando el esquema de etiquetado <a href=\"https://spacy.io/api/annotation#section-named-entities\"> BILUO</a> para describir fronteras de entidades.\n",
    "\n",
    "La \"B\" significa que el token comienza una entidad, la \"I\" siginfica que el token está dentro de una entidad o forma parte de una entidad, la \"O\" significa que el token está por fuera de la entidad, y finalmente el espacio vacío \"\" significa que no se pudo establecer una etiqueta para la entidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uc6Yim0L1mZi",
    "outputId": "17b19fc1-2383-4aba-be18-56e15e4c9381",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(European, 'B', 'NORP'), (authorities, 'O', ''), (fined, 'O', ''), (Google, 'B', 'ORG'), (a, 'O', ''), (record, 'O', ''), ($, 'B', 'MONEY'), (5.1, 'I', 'MONEY'), (billion, 'I', 'MONEY'), (on, 'O', ''), (Wednesday, 'B', 'DATE'), (for, 'O', ''), (abusing, 'O', ''), (its, 'O', ''), (power, 'O', ''), (in, 'O', ''), (the, 'O', ''), (mobile, 'O', ''), (phone, 'O', ''), (market, 'O', ''), (and, 'O', ''), (ordered, 'O', ''), (the, 'O', ''), (company, 'O', ''), (to, 'O', ''), (alter, 'O', ''), (its, 'O', ''), (practices, 'O', '')]\n"
     ]
    }
   ],
   "source": [
    "print([(X, X.ent_iob_, X.ent_type_) for X in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5jdIMYlZduO"
   },
   "source": [
    "# **En español**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kIqWqJGBaRfV",
    "outputId": "bb4d2d54-2e41-4e9a-c82f-d09b2bdb3ab0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting es-core-news-md==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_md-3.7.0/es_core_news_md-3.7.0-py3-none-any.whl (42.3 MB)\n",
      "     ---------------------------------------- 0.0/42.3 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/42.3 MB 330.3 kB/s eta 0:02:08\n",
      "     --------------------------------------- 0.1/42.3 MB 550.5 kB/s eta 0:01:17\n",
      "     ---------------------------------------- 0.2/42.3 MB 1.2 MB/s eta 0:00:36\n",
      "     ---------------------------------------- 0.4/42.3 MB 1.9 MB/s eta 0:00:22\n",
      "      --------------------------------------- 0.6/42.3 MB 2.7 MB/s eta 0:00:16\n",
      "     - -------------------------------------- 1.3/42.3 MB 4.7 MB/s eta 0:00:09\n",
      "     - -------------------------------------- 1.9/42.3 MB 5.9 MB/s eta 0:00:07\n",
      "     -- ------------------------------------- 2.2/42.3 MB 5.8 MB/s eta 0:00:07\n",
      "     --- ------------------------------------ 3.6/42.3 MB 8.6 MB/s eta 0:00:05\n",
      "     ---- ----------------------------------- 4.4/42.3 MB 9.4 MB/s eta 0:00:05\n",
      "     ---- ----------------------------------- 4.7/42.3 MB 9.6 MB/s eta 0:00:04\n",
      "     ----- ---------------------------------- 5.4/42.3 MB 9.6 MB/s eta 0:00:04\n",
      "     ------ --------------------------------- 6.8/42.3 MB 11.1 MB/s eta 0:00:04\n",
      "     ------- -------------------------------- 7.5/42.3 MB 11.5 MB/s eta 0:00:04\n",
      "     ------- -------------------------------- 8.3/42.3 MB 11.8 MB/s eta 0:00:03\n",
      "     -------- ------------------------------- 9.1/42.3 MB 12.1 MB/s eta 0:00:03\n",
      "     --------- ------------------------------ 9.9/42.3 MB 12.4 MB/s eta 0:00:03\n",
      "     --------- ----------------------------- 10.7/42.3 MB 16.4 MB/s eta 0:00:02\n",
      "     ---------- ---------------------------- 11.4/42.3 MB 16.8 MB/s eta 0:00:02\n",
      "     ----------- --------------------------- 12.2/42.3 MB 18.2 MB/s eta 0:00:02\n",
      "     ----------- --------------------------- 13.0/42.3 MB 17.7 MB/s eta 0:00:02\n",
      "     ------------ -------------------------- 13.8/42.3 MB 16.8 MB/s eta 0:00:02\n",
      "     ------------- ------------------------- 14.5/42.3 MB 16.8 MB/s eta 0:00:02\n",
      "     -------------- ------------------------ 15.3/42.3 MB 17.7 MB/s eta 0:00:02\n",
      "     -------------- ------------------------ 16.1/42.3 MB 17.3 MB/s eta 0:00:02\n",
      "     --------------- ----------------------- 16.8/42.3 MB 16.8 MB/s eta 0:00:02\n",
      "     ---------------- ---------------------- 17.5/42.3 MB 16.4 MB/s eta 0:00:02\n",
      "     ---------------- ---------------------- 18.2/42.3 MB 16.4 MB/s eta 0:00:02\n",
      "     ----------------- --------------------- 18.9/42.3 MB 16.0 MB/s eta 0:00:02\n",
      "     ------------------ -------------------- 19.6/42.3 MB 16.0 MB/s eta 0:00:02\n",
      "     ------------------ -------------------- 20.4/42.3 MB 16.0 MB/s eta 0:00:02\n",
      "     ------------------- ------------------- 21.1/42.3 MB 16.0 MB/s eta 0:00:02\n",
      "     -------------------- ------------------ 21.9/42.3 MB 16.4 MB/s eta 0:00:02\n",
      "     -------------------- ------------------ 22.7/42.3 MB 16.4 MB/s eta 0:00:02\n",
      "     --------------------- ----------------- 23.4/42.3 MB 16.4 MB/s eta 0:00:02\n",
      "     ---------------------- ---------------- 24.2/42.3 MB 16.4 MB/s eta 0:00:02\n",
      "     ----------------------- --------------- 24.9/42.3 MB 16.4 MB/s eta 0:00:02\n",
      "     ----------------------- --------------- 25.8/42.3 MB 16.4 MB/s eta 0:00:02\n",
      "     ------------------------ -------------- 26.5/42.3 MB 16.4 MB/s eta 0:00:01\n",
      "     ------------------------- ------------- 27.3/42.3 MB 16.4 MB/s eta 0:00:01\n",
      "     ------------------------- ------------- 28.1/42.3 MB 16.8 MB/s eta 0:00:01\n",
      "     -------------------------- ------------ 28.9/42.3 MB 16.8 MB/s eta 0:00:01\n",
      "     --------------------------- ----------- 29.7/42.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 30.5/42.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 31.3/42.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ----------------------------- --------- 32.1/42.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------ -------- 32.9/42.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------ -------- 33.6/42.3 MB 16.8 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 34.3/42.3 MB 16.8 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 35.1/42.3 MB 16.8 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 35.8/42.3 MB 16.8 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 36.6/42.3 MB 16.4 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 37.3/42.3 MB 16.4 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 38.1/42.3 MB 16.4 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 38.9/42.3 MB 16.4 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 39.7/42.3 MB 16.4 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 40.4/42.3 MB 16.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  41.3/42.3 MB 16.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  41.9/42.3 MB 16.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.1/42.3 MB 15.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.3/42.3 MB 15.6 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.3/42.3 MB 15.6 MB/s eta 0:00:01\n",
      "     --------------------------------------- 42.3/42.3 MB 13.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.0 in c:\\users\\jumunoz\\appdata\\local\\anaconda3\\lib\\site-packages (from es-core-news-md==3.7.0) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\jumunoz\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\jumunoz\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\jumunoz\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\jumunoz\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\jumunoz\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\jumunoz\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\jumunoz\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\jumunoz\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\jumunoz\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\jumunoz\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\jumunoz\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\jumunoz\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\jumunoz\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\jumunoz\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\jumunoz\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (1.10.8)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jumunoz\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jumunoz\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jumunoz\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\jumunoz\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\jumunoz\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (1.24.3)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\jumunoz\\appdata\\local\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jumunoz\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jumunoz\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jumunoz\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jumunoz\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\jumunoz\\appdata\\local\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\jumunoz\\appdata\\local\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\jumunoz\\appdata\\local\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\jumunoz\\appdata\\local\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\jumunoz\\appdata\\local\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jumunoz\\appdata\\local\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.1.1)\n",
      "Installing collected packages: es-core-news-md\n",
      "Successfully installed es-core-news-md-3.7.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download es_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "EaHDawK7Zcun"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import es_core_news_md\n",
    "nlp = es_core_news_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "KppTZNWQZcxg"
   },
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "nakFs1nUZc05"
   },
   "outputs": [],
   "source": [
    "texto = nlp(\"Carlos Slim Helú (Ciudad de México, 28 de enero de 1940) es un empresario e ingeniero mexicano. Es el decimocuarto hombre más rico del mundo, ya que posee bienes que ascienden a los 102 800 millones de dólares, lo cual representa alrededor del 6% del producto interno bruto de México y lo convierte en la persona más rica de México y de América Latina. Entre 2010 y 2013 fue la persona más rica del mundo según la revista Forbes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 157
    },
    "id": "TP_kaE_Rbq37",
    "outputId": "e6536076-ef59-42af-e899-928e3ac73aac"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Carlos Slim Helú\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Ciudad de México\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ", 28 de enero de 1940) es un empresario e ingeniero mexicano.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Es el decimocuarto hombre más rico del mundo, ya que posee bienes que ascienden a los 102 800 millones de dólares, lo cual representa alrededor del 6% del producto interno bruto de \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    México\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " y lo convierte en la persona más rica de \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    México\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " y de \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    América Latina\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Entre 2010 y 2013 fue la persona más rica del mundo según la revista \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Forbes\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for sent in texto.sents:\n",
    "  displacy.render(nlp(sent.text), jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
